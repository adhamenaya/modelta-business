The Evolutionary Architecture of Google: A Systematic Review of Migration Paradigms, Tooling, and Modernization Strategies (2000–2025)
1. Introduction: The Imperative of Continuous Modernization
The history of software engineering at Google is effectively a history of managing scale—not just the scale of requests per second or petabytes of storage, but the scale of the codebase itself and the human organization managing it. As of 2025, Google’s internal monolithic repository (monorepo) is estimated to house billions of lines of code, serving tens of thousands of engineers across the globe. In such an environment, the traditional enterprise model of software migration—characterized by infrequent, high-risk "big bang" upgrades—is functionally obsolete. Instead, Google has pioneered a paradigm of continuous modernization, where migration is not a project phase but a permanent operational state.
This report provides an exhaustive analysis of the architectural shifts, automated tooling, and engineering philosophies that have enabled Google to evolve its technical substrate while maintaining the velocity of a startup. We examine the transition from proprietary, monolithic cluster management (Borg) to decoupled, API-centric orchestration (Kubernetes); the shift from eventual consistency (BigTable) to global strong consistency (Spanner); and the evolution from syntactic, compiler-based refactoring (ClangMR) to probabilistic, AI-driven migration (LLM Agents).
The analysis draws upon two decades of research publications, engineering blogs, and technical whitepapers to synthesize a unified theory of Google’s engineering efficiency. It argues that Google’s ability to migrate is predicated on three foundational pillars: the "One Version" truth enforced by the monorepo, the industrialization of refactoring via Large-Scale Changes (LSC), and the abstraction of infrastructure complexity into managed, global services.
1.1 The Scale of the Problem
To understand the necessity of Google’s migration strategies, one must appreciate the magnitude of the underlying system. The codebase acts as a single organism. A change in a core C++ library, such as absl::strings, can potentially impact millions of call sites across products as diverse as Search, Maps, and YouTube. In a fragmented "polyrepo" environment, such a change would propagate slowly, creating a "dependency hell" of version mismatches. At Google, the "One Version" policy dictates that all dependencies must be compatible with the tip of the repository. This constraints the problem space: migrations must be atomic, or they must be backward-compatible and automated.
The operational overhead of this model is immense. The infrastructure must support millions of commits per year and execute billions of test cases daily. The sheer volume of code means that manual refactoring is mathematically impossible; a team cannot manually update 50,000 files. Thus, migration at Google is fundamentally a problem of automation and distributed systems, rather than mere software editing.
2. The Monolithic Repository (Piper): The Substrate of Evolution
The structural foundation of Google’s migration capability is its monolithic repository, known internally as Piper. While the industry often debates the merits of monorepos versus polyrepos, Google’s implementation provides the empirical baseline for the monorepo argument at the extreme end of the spectrum.
2.1 From Perforce to Piper: The Architecture of Scale
In the early 2000s, Google relied on a commercial version control system, Perforce. By 2011, the company was running the "busiest single Perforce server on the planet," tucked under a stairwell in Building 43. This single server supported over 12,000 users and executed millions of commands daily. However, the limits of vertical scaling were becoming apparent. TCP connection limitations, CPU bottlenecks, and the risk of a single point of failure necessitated a new architecture.
The migration to Piper was not merely a replacement of the backend but a fundamental re-architecture of how source code is stored and accessed.
	•	Storage Layer: Piper is built on top of BigTable and the Colossus file system, ensuring that repository data is replicated across multiple data centers for high availability. This eliminated the single-server bottleneck of Perforce.
	•	Consistency: Despite its distributed storage, Piper presents a single, consistent view of the "trunk" to all users, utilizing the Paxos consensus algorithm to order transactions globally. This ensures that two engineers on different continents see the same state of the universe, a critical requirement for atomic migrations.
2.2 CitC (Clients in the Cloud): Virtualizing the Workspace
A repository of 86TB (a 2016 figure, likely significantly higher in 2025) cannot be cloned to a developer's laptop. The solution was the development of Clients in the Cloud (CitC), a FUSE-based file system that fundamentally altered the mechanics of code migration.
In a traditional Git workflow, a developer performs a checkout, copying files to local disk. In the CitC workflow, the "checkout" is a virtual overlay. The developer sees the entire repository as a read-only file system, and their local changes are written to a writable overlay.
	•	Migration Efficiency: For a migration engineer, CitC is transformative. A script can traverse and "modify" thousands of files without the network latency of downloading them or the storage cost of hosting them. The file system only materializes the files that are actually being read or written.
	•	Continuous Integration: Because CitC workspaces exist in the cloud, they are instantly accessible to build systems and code review tools. A migration patch generated by a tool can be immediately tested in the cloud without ever touching a local workstation.
2.3 The "One Version" Policy and Diamond Dependencies
The strategic value of the monorepo for migration lies in the elimination of the "Diamond Dependency" problem. In a polyrepo world, Service A might depend on Library B (v1) and Library C, while Library C depends on Library B (v2). Integrating Service A and C creates a conflict.
In Piper, there is only one version of Library B: the version at the head of the trunk. This policy forces migration to happen forward. If the maintainers of Library B want to introduce a breaking change, they cannot simply release "v2" and let consumers upgrade at their leisure. They (or an automated process) must update all callers of Library B in the entire repository to work with the new API. This aligns the incentives: the author of the breaking change bears the cost of the migration, encouraging backward compatibility and automated tooling.
2.4 Trunk-Based Development as a Migration Enabler
Google utilizes "trunk-based development," where long-lived feature branches are effectively banned. Code is committed to the main branch frequently, often behind feature flags. This practice is crucial for large-scale migrations. If thousands of engineers were working on long-lived branches, a structural migration (like renaming a core class) would create catastrophic merge conflicts that would be impossible to resolve. By keeping everyone on the trunk, the "divergence" of the codebase is minimized, allowing automated tools to apply patches that merge cleanly.
3. Automated Software Evolution: The Large-Scale Change (LSC) Ecosystem
Given the constraints of the monorepo and the scale of the code, Google developed a formalized process for refactoring known as Large-Scale Changes (LSC). This process industrializes software evolution, treating code modification as a batch processing task rather than an artisanal craft.
3.1 ClangMR: MapReduce for Compilers
The primary engine for refactoring C++ code at Google is ClangMR. Traditional refactoring tools (like those found in IDEs) operate on a single translation unit and cannot scale to millions of files. Text-based tools (like sed) are dangerous for code modification because they lack semantic understanding (e.g., distinguishing between a class method and a local variable with the same name).
ClangMR bridges this gap by combining the Clang compiler frontend with the MapReduce parallel processing framework.
3.1.1 The Architecture of ClangMR
	•	The Map Phase: The system distributes the workload across thousands of nodes in a Google cluster. Each node acts as a "mapper," checking out a small subset of the codebase. It parses the C++ source files into Abstract Syntax Trees (ASTs), providing a rich, semantic representation of the code.
	•	The Analysis: The tool runs a "matcher" against the AST. For example, if the migration goal is to rename MethodA to MethodB only for subclasses of Widget, the matcher inspects the type hierarchy and identifies the correct nodes.
	•	The Reduce Phase: The nodes output the locations and nature of the required changes. The "reducer" aggregates these findings and generates the actual source code patches.
This architecture allows a single engineer to query and modify the entire C++ codebase—millions of files—in a matter of minutes.
3.2 Rosie: Managing the Review Logistics
Generating a patch for 100,000 files is the easy part. Getting it reviewed and committed without breaking the build is the hard part. A single commit touching that many files would be impossible to review (no human has the context for all subsystems) and would effectively lock the repository.
To solve this, Google built Rosie, a tool responsible for the logistics of LSCs.
	•	Splitting: Rosie takes the massive patch generated by ClangMR and splits it into smaller, atomic changelists (CLs). It utilizes the project structure to group changes by ownership.
	•	Routing: Leveraging the OWNERS file system (which defines who is responsible for which directory), Rosie routes each CL to the appropriate maintainers.
	•	Testing: Rosie orchestrates the testing of these split CLs. It runs the specific presubmit tests relevant to each change. If a test fails in Project X, the CL for Project X is held back, while the CLs for Project Y and Project Z proceed.
	•	Commit: Once reviewed and passed, Rosie automatically submits the changes.
This system effectively crowdsources the verification of migration. The migration author defines the transformation, but the distributed owners verify the correctness within their domain.
3.3 Kythe: The Semantic Map
Underpinning these tools is Kythe, a standardized, language-agnostic interchange format for code metadata. Kythe provides a complete semantic graph of the codebase, answering questions like "Who calls this function?" or "Where is this class defined?" across languages. Kythe allows Google to build refactoring tools that are not just text-aware but graph-aware. It enables cross-language analysis, which is critical in a polyglot environment where a Protocol Buffer definition (language-neutral) might generate code in C++, Java, and Go, all of which need to be updated simultaneously during a migration.
3.4 Case Study: Operation RoseHub
The efficacy of this ecosystem was demonstrated during "Operation RoseHub" in 2017. A vulnerability was discovered in the Apache Commons Collections library, affecting thousands of Java projects within Google. The vulnerability allowed remote code execution via serialization. Using the LSC infrastructure, a small team of engineers was able to:
	•	Identify all transitively dependent projects using the build graph.
	•	Generate patches to upgrade or mitigate the library usage across the entire monorepo.
	•	Split and distribute these patches via Rosie.
	•	Secure the entire codebase in a fraction of the time it would take using manual remediation.
3.5 The "Busy Beavers" and Crowdsourcing
Not all migrations can be fully automated. Some require human judgment (e.g., renaming variables for clarity or rewriting comments). For these, Google developed the Busy Beavers platform. This platform gamifies "micro-refactoring." Tasks that are too complex for a script but too small for a dedicated project are posted to a queue. Engineers looking for a break from their main project can "claim" a task, perform the clean-up, and earn points (often redeemable for charity donations or swag). This effectively utilizes the "spare cycles" of the engineering population to maintain code hygiene.
4. The AI Inflection Point: Generative Migration (2024–2025)
As of 2024 and 2025, Google has begun a significant shift from deterministic, compiler-based refactoring to probabilistic, AI-driven migration. While ClangMR is excellent for syntactic changes, it struggles with semantic migrations where the logic or framework paradigm shifts entirely.
4.1 Beyond Syntax: The Semantic Gap
Consider the migration from an x86-optimized assembly block to an Arm-compatible C++ intrinsic, or the migration from a legacy UI framework to a modern reactive framework. These changes require "invention"—understanding the intent of the code and rewriting it. AST matchers cannot easily handle this.
4.2 LLM-Assisted Migration Architectures
Recent research papers (2025) reveal that Google is deploying Large Language Models (LLMs) to bridge this gap. The architecture involves specialized agents integrated into the LSC workflow.
4.2.1 The Agentic Workflow
	•	Discovery: The system uses standard tools (Kythe, Code Search) to identify migration candidates (e.g., "Find all instances of deprecated Framework X").
	•	Prompt Engineering: The agent constructs a prompt containing the specific source code, the migration guide (documentation), and few-shot examples of successful migrations.
	•	Generation: The LLM generates the refactored code.
	•	Verification: Crucially, the system attempts to compile the code and run existing tests. If the build fails, the error message is fed back into the LLM as a new prompt ("You made this error, please fix it"), creating a self-healing loop.
4.2.2 Performance Metrics
A study of 39 distinct large-scale migrations involving x86-to-Arm and other framework updates showed profound results:
	•	Code Generation: The LLM generated 74.45% of the required edits.
	•	Time Reduction: Developers reported a 50% reduction in the total time spent on the migration compared to manual efforts.
	•	Adoption: In the first three quarters of 2024, AI-powered migrations successfully landed thousands of changelists, unblocking complex modernizations that had been stalled for years due to high toil.
4.3 Addressing Hallucinations and Safety
The integration of non-deterministic AI into the rigorous "One Version" repository introduces risks. Google mitigates this through Hybrid Tooling. The LLM is not given free rein; it operates within a sandbox where its output is subjected to the same rigorous presubmit checks as a human engineer. Furthermore, the "Review Logistics" system (Rosie) remains the final gatekeeper, ensuring that a human domain expert reviews the AI-generated code. The AI effectively acts as a "force multiplier," handling the repetitive translation work while the human focuses on architectural correctness.
4.4 Hardware Migration: x86 to Arm (Axion)
A specific driver for this AI adoption is the hardware migration to Axion, Google’s custom Arm-based datacenter processor. Migrating a codebase optimized for Intel x86 over two decades is a massive undertaking. Code containing inline assembly, SIMD intrinsics (SSE/AVX), or assumptions about memory ordering needs to be rewritten. The 2025 research highlights how AI agents are scanning the repository for architecture-specific code and suggesting Arm Neon or SVE equivalents, accelerating the ROI on custom silicon.
5. Compute Infrastructure: The Borg-Omega-Kubernetes Lineage
While the code lives in Piper, it runs on Borg. The evolution of Google’s cluster management systems—from Borg to Omega and finally to the open-source Kubernetes—represents a migration from monolithic management to decoupled, intent-based orchestration.
5.1 Borg: The Monolithic Predecessor
Developed in the early 2000s, Borg was the first unified container management system at Google. It treated a cluster of thousands of machines as a single logical computer.
	•	Architecture: Borg utilized a centralized Borgmaster to handle scheduling and a Borglet agent on each machine. It introduced the concept of "Jobs" (collections of tasks) and "Cells" (management units).
	•	Limitations: Borg had significant architectural rigidity. The scheduler was monolithic; adding a new scheduling policy required recompiling the master. More critically, Borg did not provide strong network isolation. It used a shared IP address model where tasks were assigned dynamic ports. This "port management hell" meant applications had to be written with complex logic to discover which port their dependencies were listening on, preventing the use of standard ports like 80 or 443.
5.2 Omega: The Shared State Experiment
To address the scalability limits of the Borgmaster, Google developed Omega (c. 2013).
	•	The Innovation: Omega introduced a shared-state architecture. Instead of a single master making all decisions, the cluster state was stored in a transactional store (based on Paxos). Multiple schedulers (e.g., one for batch jobs, one for long-running services) could read the state, make optimistic scheduling decisions, and attempt to commit them.
	•	Legacy: Omega proved that decoupling the scheduler from the state was possible, but it lacked a unified API for policy enforcement. It served as the intellectual bridge between Borg and Kubernetes.
5.3 Kubernetes: The Open Source Synthesis
Kubernetes (K8s) was released in 2014, not as a direct clone of Borg, but as a "second system" that fixed its architectural flaws.
	•	IP-per-Pod: Learning from the pain of Borg’s port management, Kubernetes assigned a unique IP address to every Pod. This allowed legacy applications to migrate to the cloud without rewriting their networking logic, as they could continue to bind to standard ports.
	•	Labels vs. Numbers: Borg identified tasks by sequential index (Job A, Task 0). This caused issues during updates (e.g., if Task 0 dies and restarts, is it the same task?). Kubernetes introduced Labels (key-value pairs) as the primary grouping mechanism, decoupling identity from topology. This allowed for more flexible updates and canary deployments.
	•	API-Centricity: Unlike Borg’s custom binary protocols, Kubernetes is built entirely around a RESTful API. This allowed the ecosystem of tools (Helm, Istio, etc.) to flourish, as everything is just an API object.
5.4 Internal Co-existence and Migration
It is a common misconception that Google strictly "runs on Kubernetes." In reality, Google runs on Borg, but the two systems have converged.
	•	GKE on Borg: Google Cloud’s Kubernetes Engine (GKE) is effectively a virtualization layer running on top of Borg. Borg manages the physical machines, while Kubernetes manages the logical containers for the user.
	•	Migration: Internal teams are increasingly migrating from raw Borg configuration (BCL) to Kubernetes-style patterns, adopting the benefits of the open ecosystem while retaining the scale of Borg.
6. Data Consistency and Storage Modernization
The migration of Google’s storage layer is perhaps the most ambitious in distributed systems history: a move from "Eventual Consistency" to "Strong Global Consistency," defying the traditional interpretation of the CAP Theorem.
6.1 The NoSQL Era: BigTable
In 2006, Google published the BigTable paper. BigTable was the archetypal NoSQL database: a wide-column store designed for massive throughput (e.g., storing the web index).
	•	Trade-off: To achieve scale, BigTable sacrificed complex transactions. It only supported single-row transactions. This forced application developers (like the Gmail or Ads teams) to write complex logic to handle data consistency across rows, effectively re-implementing database features in the application layer.
6.2 MegaStore: The Uncomfortable Middle Ground
To address the need for transactions, Google built MegaStore on top of BigTable. It provided ACID transactions across entity groups but suffered from poor write performance because it relied on a chatty implementation of Paxos over the wide-area network. It was a necessary step, but operationally painful.
6.3 The Spanner Revolution
Spanner (2012) represented the modernization of the entire storage stack. It is a globally distributed database that offers ACID transactions and SQL semantics.
	•	TrueTime: The technological enabler for Spanner was TrueTime, an API backed by atomic clocks and GPS receivers in every datacenter. TrueTime exposes time as an interval of uncertainty ([t_{earliest}, t_{latest}]). By waiting out the uncertainty interval (Commit Wait), Spanner guarantees that if Transaction A finished before Transaction B started, A receives a timestamp strictly less than B. This provides External Consistency without the massive communication overhead usually required for global ordering.
6.3.1 "Becoming a SQL System"
Spanner initially launched with a proprietary NoSQL-like API. However, the 2017 paper "Spanner: Becoming a SQL System" details the massive internal migration to add a standardized SQL engine.
	•	The Driver: The lack of SQL was a barrier to adoption. Developers familiar with MySQL struggled with Spanner's custom API. Furthermore, Google wanted a unified SQL dialect (GoogleSQL) across Spanner, BigQuery (Dremel), and F1.
	•	The Migration: This involved building a distributed query processor capable of "Distributed Joins" and "Cross-Apply" operations. The migration allowed legacy applications written for MySQL to be ported to Spanner with significantly less friction.
6.4 The Ads Migration (F1)
The most critical stress test for this architecture was the migration of the Google Ads database (F1) from sharded MySQL to Spanner.
	•	The Legacy State: The Ads system ran on a manually sharded MySQL cluster. Re-sharding (moving customers to different machines to balance load) was a high-risk, manual operation that took months.
	•	The Migration Strategy: Google utilized a "dual-write" strategy. The application wrote to both MySQL and Spanner/F1. A separate verification process read from both and alerted on discrepancies. Once consistency was proven over weeks, the read path was flipped to Spanner, and finally, the MySQL cluster was decommissioned. This allowed the migration of petabytes of high-value financial data with zero downtime.
7. Search and Ads Architecture: Specialized Migrations
Beyond general infrastructure, Google’s revenue-generating engines—Search and Ads—have their own history of radical architecture migration.
7.1 Search: Mustang, TeraGoogle, and Alexandria
The search index has evolved to handle the explosion of the web and the demand for real-time results.
	•	Mustang: The early architecture. It kept the entire index in RAM for low-latency retrieval. As the web grew, the cost of RAM became prohibitive.
	•	TeraGoogle: To solve the cost issue, Google introduced a tiered storage system. TeraGoogle stored the "long tail" of less popular documents on Flash (SSDs) and disk.
	•	Architecture: The serving system was split. A query would first hit Mustang (high-quality docs). If needed, it would query TeraGoogle.
	•	Data Structures: The migration involved utilizing compressed quality signals in the "PerDocData" structures to allow preliminary scoring even on the slower tiers.
	•	Alexandria: The modern system focuses on freshness. In the Mustang era, the index was often rebuilt in batches (MapReduce). Alexandria enables "incremental indexing," where a single document can be crawled, processed, and made searchable globally in seconds.
	•	Twiddlers: The ranking architecture also migrated from a monolithic scoring function to a pipeline of "Twiddlers"—lightweight re-ranking functions that run after the initial retrieval. This allows specific teams (e.g., Freshness, Local) to inject logic without modifying the core ranking engine.
7.2 Ads: The Single Request Architecture (SRA)
The Google Ad Manager (formerly DFP) migrated to a Single Request Architecture (SRA) to improve page load times and ad selection logic.
	•	The Shift: Previously, a webpage with three ad units made three separate calls to Google. This prevented holistic optimization (e.g., ensuring two car ads don't appear next to each other, known as "competitive exclusion").
	•	The Migration: SRA allows the browser to send one request defining all slots. The server runs a complex constraint satisfaction algorithm to fill the bundle. The migration to SRA was technically challenging because it moved state management from the client (browser) to the server, requiring a complete rewrite of the ad selection backend while maintaining millions of active campaigns.
8. Communication Architectures: Stubby to gRPC
The connective tissue of Google’s microservices is the Remote Procedure Call (RPC) system. The migration from Stubby to gRPC represents a shift from proprietary optimization to open standards.
8.1 Stubby: The Internal Standard
For over a decade, Google services communicated via Stubby. It was tightly integrated with Google’s load balancing and security infrastructure.
	•	The Problem: Stubby was closed source. This meant that Google Cloud customers couldn't use it, and Google’s internal teams couldn't easily integrate with open-source tools. It created a "walled garden" that hindered the hybrid cloud strategy.
8.2 gRPC: The Universal Dialect
In 2015, Google open-sourced gRPC, a rewrite of Stubby based on HTTP/2 and Protocol Buffers.
	•	Architectural Benefits:
	•	HTTP/2: Enabled multiplexing (multiple requests over one TCP connection) and header compression (HPACK), crucial for mobile performance.
	•	Polyglot: gRPC was designed from day one to support C++, Java, Go, Python, Ruby, and more, facilitating a heterogeneous service environment.
	•	The Migration: Migrating internal services from Stubby to gRPC was a massive LSC effort. It involved updating the Protocol Buffer compiler to generate gRPC stubs and systematically rolling out the new transport layer across the fleet. This was done transparently in many cases by dual-stacking the RPC libraries, allowing services to speak both Stubby and gRPC during the transition period.
9. Application Modernization Patterns & Google Cloud Guidance
The internal lessons learned from these migrations—Borg to K8s, MySQL to Spanner, Monolith to Microservices—have been codified into the Google Cloud Migration Framework, which Google advises enterprise customers to follow.
9.1 The "Strangler Fig" Pattern
Google strongly advocates for the Strangler Fig Pattern (coined by Martin Fowler) for decomposing monoliths.
	•	The Concept: Rather than a "rewrite from scratch" (which often fails), the legacy system is kept running. New features are built as microservices. An API Gateway (such as Apigee) is placed in front. Traffic is selectively routed: new paths go to the microservices, old paths go to the monolith.
	•	The Outcome: Over time, the monolith is "strangled" as its functionality is hollowed out and moved to the new architecture. This de-risks the migration by allowing incremental validation.
	•	Tooling: Google provides Migrate to Containers (formerly Migrate for Anthos) to support this. It can lift a VM-based monolith into a GKE container automatically, making it easier to orchestrate alongside the new microservices during the strangulation phase.
9.2 The 7 Rs of Migration
Google refines the industry-standard migration strategies into a "7 Rs" framework, providing a decision matrix for modernization.
Table 1: Google Cloud's 7 Rs Migration Strategy
Strategy
Definition
Technical Implication
Ideal For
Rehost
Lift and Shift
Move VMs to Compute Engine (GCE) or Google Cloud VMware Engine. Minimal code change.
Data Center exit; Speed.
Replatform
Lift and Optimize
Move to managed services (Cloud SQL) or containers (GKE) without code rewrite.
Reducing ops overhead.
Refactor
Move and Improve
Modify code to use cloud-native features (e.g., replace file logging with Cloud Logging).
Improving reliability/scale.
Re-architect
Modernize
Decompose monolith to microservices; switch DB to Spanner.
Business agility; Innovation.
Rebuild
Rip and Replace
Discard legacy code; write from scratch using Cloud Native technologies.
Legacy tech debt is too high.
Repurchase
SaaS
Switch to a SaaS provider (e.g., G Suite/Workspace).
Commodity functions (Email, HR).
Retire
Decommission
Turn off the system.
Unused workloads.
9.3 Case Study: Twitter's Migration to GCP
A prominent public example of these patterns is Twitter’s migration of its ad analytics platform to Google Cloud.
	•	The Challenge: Twitter had a massive on-premise Hadoop/LZO pipeline and a custom key-value store called Manhattan. The maintenance burden was high.
	•	The Migration: They migrated to Dataflow (managed Beam) and BigQuery.
	•	Key Insight: Instead of lifting the Hadoop clusters (Rehost), they re-architected the pipelines to use the cloud-native stream processing model of Dataflow, achieving lower latency and higher reliability. This mirrors Google’s own internal migration from MapReduce to Flume (the internal precursor to Dataflow).
10. Conclusion: The Future of Engineering
Google’s engineering history is a testament to the idea that architecture is never final. The migration from Borg to Kubernetes, from BigTable to Spanner, and from Stubby to gRPC demonstrates a willingness to cannibalize successful systems in pursuit of greater scalability and developer velocity.
The central nervous system of this evolution is the Monorepo, which transforms the codebase into a unified data structure that can be queried, refactored, and evolved atomically. The LSC ecosystem (ClangMR, Rosie) provides the mechanical muscle to execute these changes, while the culture of Trunk-Based Development ensures the code never diverges too far from the bleeding edge.
As we look toward 2030, the integration of Generative AI marks the next phase. With tools that can semantically understand and migrate code with 75% automation, Google is moving toward a future where "legacy code" is not a permanent liability, but a temporary state that can be continuously modernized by intelligent agents. The barrier to migration is lowering, shifting the engineer’s role from "code editor" to "evolution architect."
Summary of Key Architectural Shifts
Table 2: The Evolution of Google's Technology Stack
Domain
Legacy / Internal Predecessor
Modern / Open Standard
The Migration Driver
Compute
Borg (Monolithic)
Kubernetes (Decoupled)
Port isolation, API extensibility, ecosystem.
Storage
BigTable (Eventual Consistency)
Spanner (Strong Consistency)
Transactional integrity, simplified app logic.
Networking
Stubby (Proprietary RPC)
gRPC (HTTP/2 RPC)
Interoperability, polyglot support, streaming.
Data Processing
MapReduce (Batch)
Dataflow / Flume (Streaming)
Latency, unified batch/stream model.
Refactoring
Manual / Sed (Textual)
ClangMR / LLM Agents (Semantic)
Scale of codebase, complexity of migration.
Versioning
Perforce (Centralized)
Piper / CitC (Distributed)
12k+ users, 86TB+ repository size.
This continuous cycle of invention, migration, and standardization defines the "Google way" of engineering—a model where change is the only constant, and the infrastructure is designed to survive it.
Works cited
1. Google Monorepo Paper | The Uncommon Engineer, https://ronamosa.io/docs/books/papers/Google3/ 2. Why Google Stores Billions of Lines of Code in a Single Repository, https://cacm.acm.org/research/why-google-stores-billions-of-lines-of-code-in-a-single-repository/ 3. Software Engineering at Google: Lessons Learned from Programming Over Time [1 ed.] 1492082791, 9781492082798 - DOKUMEN.PUB, https://dokumen.pub/software-engineering-at-google-lessons-learned-from-programming-over-time-1nbsped-1492082791-9781492082798.html 4. How Google migrated billions of lines of code from Perforce to Piper - Graphite, https://graphite.com/blog/google-perforce-to-piper-migration 5. Google DeepMind Gemini – Dr Alan D. Thompson - LifeArchitect.ai, https://lifearchitect.ai/gemini/ 6. Large-Scale Automated Refactoring Using ClangMR - Google Research, https://research.google.com/pubs/archive/41342.pdf 7. Large-Scale Automated Refactoring Using ClangMR - ResearchGate, https://www.researchgate.net/publication/261417153_Large-Scale_Automated_Refactoring_Using_ClangMR 8. Large-Scale Changes - Software Engineering at Google, https://abseil.io/resources/swe-book/html/ch22.html 9. Accelerating code migrations with AI - Google Research, https://research.google/blog/accelerating-code-migrations-with-ai/ 10. SE Radio 609: Hyrum Wright on Software Engineering at Google, https://se-radio.net/2024/03/se-radio-609-hyrum-wright-on-software-engineering-at-google/ 11. Migrating Code At Scale With LLMs At Google - arXiv, https://arxiv.org/html/2504.09691v1 12. Instruction Set Migration at Warehouse Scale - arXiv, https://arxiv.org/html/2510.14928v1 13. How is Google using AI for internal code migrations? - arXiv, https://arxiv.org/html/2501.06972v1 14. Large-scale cluster management at Google with Borg, https://research.google.com/pubs/archive/43438.pdf 15. Borg, Omega, and Kubernetes - Google Research, https://research.google.com/pubs/archive/44843.pdf 16. The Technical History of Kubernetes | by Brian Grant - ITNEXT, https://itnext.io/the-technical-history-of-kubernetes-2fe1988b522a 17. The History of Kubernetes | IBM, https://www.ibm.com/think/topics/kubernetes-history 18. (PDF) Research Presentation Google Borg - ResearchGate, https://www.researchgate.net/publication/366185651_Research_Presentation_Google_Borg 19. How Kubernetes came to be: A co-founder shares the story | Google Cloud Blog, https://cloud.google.com/blog/products/containers-kubernetes/from-google-to-the-world-the-kubernetes-origin-story 20. Spanner: Google's Globally-Distributed Database, https://research.google.com/archive/spanner-osdi2012.pdf 21. What Goes Around Comes Around... And Around... - Khoury College of Computer Sciences, https://khoury.northeastern.edu/home/pandey/courses/cs7270/fall25/papers/intro/whatgoesaroundaround-stonebraker.pdf 22. CMU Talk - Spanner and the Evolution of its Storage Engine (2024-11-13), https://www.cs.cmu.edu/~15721-f24/slides/18-Spanner.pdf 23. David F. Bacon - Google Research, https://research.google/people/davidfbacon/ 24. Spanner: Becoming a SQL System - Google Research, https://research.google.com/pubs/archive/46103.pdf 25. Whitepapers | Spanner - Google Cloud Documentation, https://docs.cloud.google.com/spanner/docs/whitepapers 26. API Content Warehouse Part03 | PDF | Boolean Data Type | Json - Scribd, https://www.scribd.com/document/880845128/API-Content-Warehouse-Part03 27. Google Ranking Signals - Dixon Jones, https://dixonjones.com/google-ranking-signals/ 28. Google's Leaked CompressedQualitySignals: Advanced SEO Analysis - Hobo, https://www.hobo-web.co.uk/compressedqualitysignals/ 29. The PerDocData: Google's Leaked Core Document Model - Hobo, https://www.hobo-web.co.uk/perdocdata/ 30. Great News for SEOs : Google 2024 Algorithm Leaked | Dorve, https://dorve.com/seo/google-2024-algorithm-leaked-seos/ 31. Migrating to Google's Single Request Architecture (SRA) | by Tripadvisor - Medium, https://medium.com/tripadvisor/migrating-to-googles-single-request-architecture-sra-4073daca26ad 32. gRPC Decoded: The API Protocol That's Changing Everything - Tailcall, https://tailcall.run/blog/what-is-grpc/ 33. What is gRPC? Use Cases and Benefits - Kong Inc., https://konghq.com/blog/learning-center/what-is-grpc 34. gRPC Motivation and Design Principles, https://grpc.io/blog/principles/ 35. An Introduction to gRPC - Mattermost, https://mattermost.com/blog/an-introduction-to-grpc/ 36. Re-architecting To Cloud Native, https://cloud.google.com/resources/rearchitecting-to-cloud-native 37. How To Use The Strangler Fig Pattern for Application Modernization - vFunction, https://vfunction.com/resources/guide-how-to-use-the-strangler-fig-pattern-for-application-modernization/ 38. Database Modernization - Google Cloud, https://cloud.google.com/solutions/database-modernization 39. Migrate to Google Cloud: Get started | Cloud Architecture Center ..., https://docs.cloud.google.com/architecture/migration-to-gcp-getting-started 40. Twitter's data transformation pipelines for ads | Google Cloud Blog, https://cloud.google.com/blog/products/data-analytics/modernizing-twitters-ad-engagement-analytics-platform
